{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(range_):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for a in range(range_):\n",
    "        v = np.random.random(2)\n",
    "        targets.append(sum(v))\n",
    "        inputs.append(v)\n",
    "    return inputs,targets\n",
    "\n",
    "inputs,targets = data_generator(200)\n",
    "print(inputs[0],targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, input, hiddens, output, learning_rate):\n",
    "        self.input = input\n",
    "        self.hiddens = hiddens\n",
    "        self.output = output\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        self.layers = [input] + hiddens + [output]\n",
    "\n",
    "        weights = []\n",
    "        biases = []\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            weights.append(np.random.randn(self.layers[i], self.layers[i + 1]) * np.sqrt(2 / self.layers[i]))\n",
    "            biases.append(np.zeros(self.layers[i + 1]))\n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward_pass(self, inputs):\n",
    "        activations = inputs\n",
    "        for index, (weight, bias) in enumerate(zip(self.weights, self.biases)):\n",
    "            net_inputs = np.dot(activations, weight) + bias\n",
    "            if index != len(self.weights) - 1:\n",
    "                activations = self.activation(net_inputs)\n",
    "            else:\n",
    "                activations = net_inputs\n",
    "        return activations\n",
    "\n",
    "    def activation(self, inputs):\n",
    "        return np.maximum(0, inputs)\n",
    "\n",
    "    def activation_derivative(self, inputs):\n",
    "        return (inputs > 0).astype(float)\n",
    "\n",
    "    def backpropagation(self, error, input_):\n",
    "        gradients_weights = [np.zeros_like(weight) for weight in self.weights]\n",
    "        gradients_biases = [np.zeros_like(bias) for bias in self.biases]\n",
    "\n",
    "        delta = error\n",
    "        for i in range(len(self.weights) - 1, -1, -1):\n",
    "            activations = self.activation(self.forward_pass(input_) if i == 0 else np.dot(self.activations[i - 1], self.weights[i - 1]))\n",
    "            gradients_weights[i] = np.outer(self.activations[i], delta)\n",
    "            gradients_biases[i] = delta\n",
    "            delta = np.dot(delta, self.weights[i].T) * self.activation_derivative(self.activations[i])\n",
    "\n",
    "        return gradients_weights, gradients_biases\n",
    "\n",
    "    def train(self, epochs, inputs, targets):\n",
    "        for epoch in range(epochs):\n",
    "            sum_errors = 0\n",
    "            for i, input_ in enumerate(inputs):\n",
    "                target = targets[i]\n",
    "                output = self.forward_pass(input_)\n",
    "                error = target - output\n",
    "\n",
    "                self.activations = [input_] + [self.activation(np.dot(input_, self.weights[0]) + self.biases[0])]\n",
    "                for j in range(1, len(self.weights) - 1):\n",
    "                    self.activations.append(self.activation(np.dot(self.activations[j], self.weights[j]) + self.biases[j]))\n",
    "\n",
    "                gradients_weights, gradients_biases = self.backpropagation(error, input_)\n",
    "\n",
    "                for j, (grad_weight, grad_bias) in enumerate(zip(gradients_weights, gradients_biases)):\n",
    "                    self.weights[j] += self.learning_rate * grad_weight\n",
    "                    self.biases[j] += self.learning_rate * grad_bias\n",
    "\n",
    "                mean_squared_error = np.average(error ** 2)\n",
    "                sum_errors += mean_squared_error\n",
    "\n",
    "            print(\"Error: {} at epoch {}\".format(sum_errors / len(inputs), epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input=2, hiddens=[2], output=1, learning_rate=0.01)\n",
    "model.train(epochs=100, inputs=inputs, targets=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward_pass([0.2,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
